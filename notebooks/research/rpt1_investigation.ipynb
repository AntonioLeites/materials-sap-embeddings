{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6568cc56",
   "metadata": {},
   "source": [
    "# SAP RPT-1 Investigation\n",
    " \n",
    "**Goal:** Understand how to use RPT-1 as an encoder for material embeddings\n",
    "\n",
    "**Key Questions:**\n",
    "1. How to initialize RPT-1?\n",
    "2. Can we extract embeddings without training?\n",
    "3. What's the structure of tokenized data?\n",
    "4. How to integrate with our multimodal pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae325a36",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f5cedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/antonio/Documents/Herramientas SAP/ML/RPT-1/materials-sap-embeddings\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cdb9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import RPT-1\n",
    "from sap_rpt_oss import SAP_RPT_OSS_Classifier\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be37ca6",
   "metadata": {},
   "source": [
    "## 2. Initialize RPT-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87e498cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start async server to compute embedding on port 5655\n",
      "Port already in use, not starting again.\n",
      "‚úì Classifier initialized\n",
      "  Type: <class 'sap_rpt_oss.rpt.SAP_RPT_OSS_Classifier'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize with lightweight config for exploration\n",
    "classifier = SAP_RPT_OSS_Classifier(\n",
    "    bagging=1,              # Single model (faster)\n",
    "    max_context_size=2048,  # Smaller context (faster)\n",
    ")\n",
    "\n",
    "print(\"‚úì Classifier initialized\")\n",
    "print(f\"  Type: {type(classifier)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cefd5e",
   "metadata": {},
   "source": [
    "## 3. Explore Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfcc6914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found tokenizer: <class 'sap_rpt_oss.data.tokenizer.Tokenizer'>\n",
      "\n",
      "Tokenizer attributes:\n",
      "  - QUANTILE_DIMENSION\n",
      "  - classification_type\n",
      "  - embedding_dim\n",
      "  - is_valid\n",
      "  - num_regression_bins\n",
      "  - random_seed\n",
      "  - regression_type\n",
      "  - sentence_embedding_model_name\n",
      "  - socket\n",
      "  - zmq_port\n",
      "‚úì Found model: <class 'sap_rpt_oss.model.torch_model.RPT'>\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Check for tokenizer\n",
    "if hasattr(classifier, 'tokenizer'):\n",
    "    tokenizer = classifier.tokenizer\n",
    "    print(f\"‚úì Found tokenizer: {type(tokenizer)}\")\n",
    "    \n",
    "    # Explore tokenizer\n",
    "    print(\"\\nTokenizer attributes:\")\n",
    "    for attr in dir(tokenizer):\n",
    "        if not attr.startswith('_') and not callable(getattr(tokenizer, attr, None)):\n",
    "            print(f\"  - {attr}\")\n",
    "\n",
    "# %%\n",
    "# Check for estimators\n",
    "attrs_to_check = ['estimators', 'estimators_', 'estimator', 'estimator_', 'model']\n",
    "\n",
    "for attr in attrs_to_check:\n",
    "    if hasattr(classifier, attr):\n",
    "        val = getattr(classifier, attr)\n",
    "        print(f\"‚úì Found {attr}: {type(val)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab039055",
   "metadata": {},
   "source": [
    "## 4. Create Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af00c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MATNR</th>\n",
       "      <th>MAKTX</th>\n",
       "      <th>MATKL</th>\n",
       "      <th>MTART</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>NUM_PLANTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAT001</td>\n",
       "      <td>Steel Bolt M8x50 DIN 933</td>\n",
       "      <td>BOLTS</td>\n",
       "      <td>FERT</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAT002</td>\n",
       "      <td>Plastic Washer M8</td>\n",
       "      <td>WASHERS</td>\n",
       "      <td>FERT</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MAT003</td>\n",
       "      <td>Stainless Steel Nut M6</td>\n",
       "      <td>NUTS</td>\n",
       "      <td>FERT</td>\n",
       "      <td>0.15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MATNR                     MAKTX    MATKL MTART  PRICE  NUM_PLANTS\n",
       "0  MAT001  Steel Bolt M8x50 DIN 933    BOLTS  FERT   0.50           3\n",
       "1  MAT002         Plastic Washer M8  WASHERS  FERT   0.10           2\n",
       "2  MAT003    Stainless Steel Nut M6     NUTS  FERT   0.15           4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Material-like data\n",
    "df_test = pd.DataFrame({\n",
    "    'MATNR': ['MAT001', 'MAT002', 'MAT003'],\n",
    "    'MAKTX': [\n",
    "        'Steel Bolt M8x50 DIN 933',\n",
    "        'Plastic Washer M8',\n",
    "        'Stainless Steel Nut M6'\n",
    "    ],\n",
    "    'MATKL': ['BOLTS', 'WASHERS', 'NUTS'],\n",
    "    'MTART': ['FERT', 'FERT', 'FERT'],\n",
    "    'PRICE': [0.50, 0.10, 0.15],\n",
    "    'NUM_PLANTS': [3, 2, 4],\n",
    "})\n",
    "\n",
    "print(\"Test data:\")\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680df94",
   "metadata": {},
   "source": [
    "## 5. Attempt: Use Tokenizer Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29a0c456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting direct tokenization...\n",
      "Tokenizer methods: ['build_labels', 'convert_type_', 'process_features', 'process_target', 'quantize_column', 'replace_inf_values', 'socket_init', 'standard_scale_column', 'texts_to_tensor', 'time_to_seconds']\n",
      "‚úó Error: Tokenizer.__call__() missing 4 required positional arguments: 'y_context', 'X_query', 'y_query', and 'classification_or_regression'\n"
     ]
    }
   ],
   "source": [
    "# Try to tokenize without fitting\n",
    "if hasattr(classifier, 'tokenizer'):\n",
    "    tokenizer = classifier.tokenizer\n",
    "    \n",
    "    print(\"Attempting direct tokenization...\")\n",
    "    \n",
    "    try:\n",
    "        # See tokenizer methods\n",
    "        methods = [m for m in dir(tokenizer) if not m.startswith('_') and callable(getattr(tokenizer, m))]\n",
    "        print(f\"Tokenizer methods: {methods[:10]}\")\n",
    "        \n",
    "        # Try tokenize method if it exists\n",
    "        if hasattr(tokenizer, 'tokenize'):\n",
    "            result = tokenizer.tokenize(df_test)\n",
    "            print(f\"‚úì Tokenization successful!\")\n",
    "            print(f\"  Type: {type(result)}\")\n",
    "            \n",
    "        elif hasattr(tokenizer, '__call__'):\n",
    "            result = tokenizer(df_test)\n",
    "            print(f\"‚úì Tokenization successful (via __call__)!\")\n",
    "            print(f\"  Type: {type(result)}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73e1cb",
   "metadata": {},
   "source": [
    " ## 6. Attempt: Fit on Dummy Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76cb1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping MATKL:\n",
      "  Columns: ['MATNR', 'MAKTX', 'MATKL', 'MTART', 'PRICE', 'NUM_PLANTS']\n",
      "  Has duplicate columns: False\n",
      "\n",
      "After dropping MATKL:\n",
      "  Columns: ['MATNR', 'MAKTX', 'MTART', 'PRICE', 'NUM_PLANTS']\n",
      "  Has duplicate columns: False\n",
      "\n",
      "Training data:\n",
      "  X shape: (3, 5)\n",
      "  X index: [0, 1, 2]\n",
      "  X columns: ['MATNR', 'MAKTX', 'MTART', 'PRICE', 'NUM_PLANTS']\n",
      "  y unique values: ['BOLTS' 'WASHERS' 'NUTS']\n",
      "Fitting classifier...\n",
      "‚úì Classifier fitted!\n",
      "  X_ shape: (3, 5)\n",
      "  X_ columns: ['MATNR', 'MAKTX', 'MTART', 'PRICE', 'NUM_PLANTS']\n",
      "  X_ has unique columns: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Strategy: Fit on a simple supervised task to initialize the model\n",
    "# Then we can extract embeddings\n",
    "\n",
    "# Create target: predict MaterialGroup\n",
    "df_train = df_test.copy()\n",
    "\n",
    "# CRITICAL: Check for duplicate columns BEFORE dropping\n",
    "print(\"Before dropping MATKL:\")\n",
    "print(f\"  Columns: {df_train.columns.tolist()}\")\n",
    "print(f\"  Has duplicate columns: {not df_train.columns.is_unique}\")\n",
    "\n",
    "y_train = df_train['MATKL']  # Use MaterialGroup as target\n",
    "X_train = df_train.drop('MATKL', axis=1)\n",
    "\n",
    "# Check again after dropping\n",
    "print(\"\\nAfter dropping MATKL:\")\n",
    "print(f\"  Columns: {X_train.columns.tolist()}\")\n",
    "print(f\"  Has duplicate columns: {not X_train.columns.is_unique}\")\n",
    "\n",
    "# If there are duplicates, remove them\n",
    "if not X_train.columns.is_unique:\n",
    "    print(\"‚ö†Ô∏è Found duplicate columns, removing...\")\n",
    "    X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
    "    print(f\"  Cleaned columns: {X_train.columns.tolist()}\")\n",
    "\n",
    "# Reset index\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nTraining data:\")\n",
    "print(f\"  X shape: {X_train.shape}\")\n",
    "print(f\"  X index: {X_train.index.tolist()}\")\n",
    "print(f\"  X columns: {X_train.columns.tolist()}\")\n",
    "print(f\"  y unique values: {y_train.unique()}\")\n",
    "\n",
    "# %%\n",
    "# Fit the classifier\n",
    "print(\"Fitting classifier...\")\n",
    "try:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"‚úì Classifier fitted!\")\n",
    "    \n",
    "    # Check stored data\n",
    "    if hasattr(classifier, 'X_'):\n",
    "        print(f\"  X_ shape: {classifier.X_.shape}\")\n",
    "        print(f\"  X_ columns: {classifier.X_.columns.tolist()}\")\n",
    "        print(f\"  X_ has unique columns: {classifier.X_.columns.is_unique}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Fitting failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70b294f",
   "metadata": {},
   "source": [
    "## 7. Extract Embeddings After Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf18c661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Test data prepared:\n",
      "  Shape: (3, 5)\n",
      "  Columns: ['MATNR', 'MAKTX', 'MTART', 'PRICE', 'NUM_PLANTS']\n",
      "  Index: [0, 1, 2]\n",
      "  Columns match training data: True\n",
      "\n",
      "Attempting to get tokenized data...\n",
      "‚úì Got tokenized data!\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Match columns exactly with training data\n",
    "df_test_clean = df_test.copy()\n",
    "\n",
    "# Remove MATKL if it exists (to match X_train structure)\n",
    "if 'MATKL' in df_test_clean.columns:\n",
    "    df_test_clean = df_test_clean.drop('MATKL', axis=1)\n",
    "\n",
    "# Remove duplicate columns if any\n",
    "if not df_test_clean.columns.is_unique:\n",
    "    print(\"‚ö†Ô∏è Removing duplicate columns from test data\")\n",
    "    df_test_clean = df_test_clean.loc[:, ~df_test_clean.columns.duplicated()]\n",
    "\n",
    "# Reset index\n",
    "df_test_clean = df_test_clean.reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Test data prepared:\")\n",
    "print(f\"  Shape: {df_test_clean.shape}\")\n",
    "print(f\"  Columns: {df_test_clean.columns.tolist()}\")\n",
    "print(f\"  Index: {df_test_clean.index.tolist()}\")\n",
    "\n",
    "# Verify columns match\n",
    "if hasattr(classifier, 'X_'):\n",
    "    columns_match = df_test_clean.columns.equals(classifier.X_.columns)\n",
    "    print(f\"  Columns match training data: {columns_match}\")\n",
    "    \n",
    "    if not columns_match:\n",
    "        print(\"  ‚ö†Ô∏è Column mismatch!\")\n",
    "        print(f\"    Train columns: {classifier.X_.columns.tolist()}\")\n",
    "        print(f\"    Test columns:  {df_test_clean.columns.tolist()}\")\n",
    "\n",
    "# Now try get_tokenized_data\n",
    "if hasattr(classifier, 'X_'):\n",
    "    print(\"\\nAttempting to get tokenized data...\")\n",
    "    \n",
    "    try:\n",
    "        tokenized = classifier.get_tokenized_data(df_test_clean, bagging_index=0)\n",
    "        \n",
    "        print(f\"‚úì Got tokenized data!\")\n",
    "        # ... rest of the cell\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5701b",
   "metadata": {},
   "source": [
    "##  8. Extract Material-Level Embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b64dcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ## \n",
    "\n",
    "# %%\n",
    "if 'tokenized' in locals() and isinstance(tokenized, dict) and 'text_embeddings' in tokenized:\n",
    "    text_emb = tokenized['text_embeddings']\n",
    "    print(f\"Text embeddings shape: {text_emb.shape}\")\n",
    "    print(f\"Expected: (n_materials, n_columns, embedding_dim)\")\n",
    "    \n",
    "    # Strategy 1: Mean pooling over columns\n",
    "    material_embeddings_mean = text_emb.mean(dim=1)\n",
    "    print(f\"\\n‚úì Mean pooled embeddings: {material_embeddings_mean.shape}\")\n",
    "    print(f\"  First material, first 10 dims:\")\n",
    "    print(f\"  {material_embeddings_mean[0, :10].detach().numpy()}\")\n",
    "    \n",
    "    # Strategy 2: Use last column (like [CLS] token)\n",
    "    material_embeddings_cls = text_emb[:, -1, :]\n",
    "    print(f\"\\n‚úì [CLS]-like embeddings: {material_embeddings_cls.shape}\")\n",
    "    print(f\"  First material, first 10 dims:\")\n",
    "    print(f\"  {material_embeddings_cls[0, :10].detach().numpy()}\")\n",
    "    \n",
    "    # Compare both\n",
    "    print(f\"\\nüìä Comparison:\")\n",
    "    print(f\"  Mean pooling L2 norm: {torch.norm(material_embeddings_mean[0]).item():.4f}\")\n",
    "    print(f\"  [CLS]-like L2 norm: {torch.norm(material_embeddings_cls[0]).item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd38c7",
   "metadata": {},
   "source": [
    "## 9. Compute Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc7c8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'material_embeddings_mean' in locals():\n",
    "    # Compute cosine similarity between materials\n",
    "    from torch.nn.functional import cosine_similarity\n",
    "    \n",
    "    # Material 0 vs Material 1\n",
    "    sim_0_1 = cosine_similarity(\n",
    "        material_embeddings_mean[0].unsqueeze(0),\n",
    "        material_embeddings_mean[1].unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    # Material 0 vs Material 2\n",
    "    sim_0_2 = cosine_similarity(\n",
    "        material_embeddings_mean[0].unsqueeze(0),\n",
    "        material_embeddings_mean[2].unsqueeze(0)\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ Similarity results:\")\n",
    "    print(f\"  {df_test.iloc[0]['MAKTX']}\")\n",
    "    print(f\"  vs\")\n",
    "    print(f\"  {df_test.iloc[1]['MAKTX']}\")\n",
    "    print(f\"  ‚Üí Similarity: {sim_0_1.item():.4f}\")\n",
    "    print()\n",
    "    print(f\"  {df_test.iloc[0]['MAKTX']}\")\n",
    "    print(f\"  vs\")\n",
    "    print(f\"  {df_test.iloc[2]['MAKTX']}\")\n",
    "    print(f\"  ‚Üí Similarity: {sim_0_2.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ec598",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "823cbe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KEY FINDINGS\n",
      "============================================================\n",
      "\n",
      "‚úì RPT-1 can be used as an encoder\n",
      "‚úì Requires fitting on a supervised task first\n",
      "‚úì Embeddings accessible via get_tokenized_data()\n",
      "‚úì text_embeddings shape: (n_materials, n_columns, embedding_dim)\n",
      "‚úì Can extract material-level embeddings via pooling\n",
      "\n",
      "NEXT STEPS:\n",
      "1. Create SAPRPT1Encoder class\n",
      "2. Integrate with MultimodalMaterialEmbeddings\n",
      "3. Compare: Current encoders vs RPT-1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"‚úì RPT-1 can be used as an encoder\")\n",
    "print(\"‚úì Requires fitting on a supervised task first\")\n",
    "print(\"‚úì Embeddings accessible via get_tokenized_data()\")\n",
    "print(\"‚úì text_embeddings shape: (n_materials, n_columns, embedding_dim)\")\n",
    "print(\"‚úì Can extract material-level embeddings via pooling\")\n",
    "print()\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Create SAPRPT1Encoder class\")\n",
    "print(\"2. Integrate with MultimodalMaterialEmbeddings\")\n",
    "print(\"3. Compare: Current encoders vs RPT-1\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d7cc1",
   "metadata": {},
   "source": [
    "## Appendix: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ee924dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FINAL helper function...\n",
      "‚úÖ Helper function works!\n",
      "  Input materials: 3\n",
      "  Output shape: torch.Size([3, 384])\n",
      "  Embedding dimension: 384\n",
      "\n",
      "  First material embedding (first 10 dims):\n",
      "    [-0.2426   -0.0096   -0.1543   -0.00938  -0.001734 -0.03326   0.1707\n",
      "  0.0841    0.012146 -0.1013  ]\n",
      "\n",
      "‚úì Sample count verified: 3 materials\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "## ## Appendix: Helper Functions (FINAL)\n",
    "\n",
    "# %%\n",
    "def extract_rpt1_embeddings(classifier, df, pooling='mean'):\n",
    "    \"\"\"\n",
    "    Extract embeddings from fitted RPT-1 classifier\n",
    "    \n",
    "    Args:\n",
    "        classifier: Fitted SAP_RPT_OSS_Classifier\n",
    "        df: DataFrame with materials (can include target column)\n",
    "        pooling: 'mean' or 'cls'\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_materials, embedding_dim)\n",
    "    \"\"\"\n",
    "    # Prepare DataFrame\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remove target column if exists\n",
    "    if 'MATKL' in df_clean.columns:\n",
    "        df_clean = df_clean.drop('MATKL', axis=1)\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    if not df_clean.columns.is_unique:\n",
    "        df_clean = df_clean.loc[:, ~df_clean.columns.duplicated()]\n",
    "    \n",
    "    # Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    # Get tokenized data\n",
    "    tokenized = classifier.get_tokenized_data(df_clean, bagging_index=0)\n",
    "    \n",
    "    # Extract text embeddings (inside 'data' dict)\n",
    "    text_emb = tokenized['data']['text_embeddings']\n",
    "    \n",
    "    # Apply pooling over columns (dim=1)\n",
    "    if pooling == 'mean':\n",
    "        pooled = text_emb.mean(dim=1)  # (n_train + n_test, embedding_dim)\n",
    "    elif pooling == 'cls':\n",
    "        pooled = text_emb[:, -1, :]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pooling: {pooling}\")\n",
    "    \n",
    "    # CRITICAL: Extract only test samples (last n_materials rows)\n",
    "    # RPT-1 concatenates [train, test], we only want test\n",
    "    n_materials = len(df_clean)\n",
    "    embeddings_test = pooled[-n_materials:]\n",
    "    \n",
    "    return embeddings_test\n",
    "\n",
    "# Test\n",
    "print(\"Testing FINAL helper function...\")\n",
    "if 'classifier' in locals() and hasattr(classifier, 'X_'):\n",
    "    embeddings = extract_rpt1_embeddings(classifier, df_test, pooling='mean')\n",
    "    \n",
    "    print(f\"‚úÖ Helper function works!\")\n",
    "    print(f\"  Input materials: {len(df_test)}\")\n",
    "    print(f\"  Output shape: {embeddings.shape}\")\n",
    "    print(f\"  Embedding dimension: {embeddings.shape[1]}\")\n",
    "    print(f\"\\n  First material embedding (first 10 dims):\")\n",
    "    print(f\"    {embeddings[0, :10].detach().numpy()}\")\n",
    "    \n",
    "    # Verify correct number of samples\n",
    "    assert embeddings.shape[0] == len(df_test), \"Sample count mismatch!\"\n",
    "    print(f\"\\n‚úì Sample count verified: {embeddings.shape[0]} materials\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Classifier not fitted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "materials-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
